{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Knowledge Graph Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from aips import *\n",
    "import os\n",
    "from IPython.core.display import display,HTML\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col\n",
    "spark = SparkSession.builder.appName(\"ch7\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def create_reviews_collection():\n",
    "    #Create Reviews Collection\n",
    "    reviews_collection=\"reviews\"\n",
    "\n",
    "    #delete collection\n",
    "    requests.get(solr_url + \"admin/collections?action=DELETE&name=\" + reviews_collection)\n",
    "    #delete configSet to start from scratch\n",
    "    requests.get(solr_url + \"admin/configs?action=DELETE&name=\" + reviews_collection + \".AUTOCREATED\")\n",
    "\n",
    "    create_collection(reviews_collection)\n",
    "    #add_text_tagger_fields(reviews_collection)\n",
    "\n",
    "    headers={\"Content-type\": \"application/json\"}\n",
    "\n",
    "    schemaCommands = [\n",
    "                \"\"\"{\n",
    "              \"add-field\":{\"name\":\"doc_type\", \"type\":\"string\", \"stored\":true, \"multiValued\": true}\n",
    "            }\"\"\",\n",
    "                \"\"\"{\n",
    "              \"add-copy-field\":{\"source\":\"type_ss\", \"dest\":[\"doc_type\"]}\n",
    "            }\"\"\",\n",
    "                    \"\"\"{\n",
    "              \"add-field\":{\"name\":\"location_p\", \"type\":\"location\", \"stored\":true}\n",
    "            }\"\"\",\n",
    "                \"\"\"{\n",
    "              \"add-copy-field\":{\"source\":\"location_pt_s\", \"dest\":[\"location_p\"]}\n",
    "            }\"\"\"\n",
    "    ]\n",
    "\n",
    "    for schemaCommand in schemaCommands:\n",
    "        response = requests.post(solr_url + reviews_collection + \"/schema\", headers=headers, data=schemaCommand)\n",
    "        #print(response)   \n",
    "        \n",
    "def enable_text_tagger(collection):\n",
    "    headers={\"Content-type\": \"application/json\"}\n",
    "    \n",
    "    schemaCommands = [\n",
    "        \"\"\"{\n",
    "          \"add-field-type\":{\n",
    "            \"name\":\"tag\",\n",
    "            \"class\":\"solr.TextField\",\n",
    "            \"postingsFormat\":\"FST50\",\n",
    "            \"omitNorms\":true,\n",
    "            \"omitTermFreqAndPositions\":true,\n",
    "            \"indexAnalyzer\":{\n",
    "              \"tokenizer\":{\n",
    "                 \"class\":\"solr.StandardTokenizerFactory\" },\n",
    "              \"filters\":[\n",
    "                {\"class\":\"solr.EnglishPossessiveFilterFactory\"},\n",
    "                {\"class\":\"solr.ASCIIFoldingFilterFactory\"},\n",
    "                {\"class\":\"solr.LowerCaseFilterFactory\"},\n",
    "                {\"class\":\"solr.ConcatenateGraphFilterFactory\", \"preservePositionIncrements\":false }\n",
    "              ]},\n",
    "            \"queryAnalyzer\":{\n",
    "              \"tokenizer\":{\n",
    "                 \"class\":\"solr.StandardTokenizerFactory\" },\n",
    "              \"filters\":[\n",
    "                {\"class\":\"solr.EnglishPossessiveFilterFactory\"},\n",
    "                {\"class\":\"solr.ASCIIFoldingFilterFactory\"},\n",
    "                {\"class\":\"solr.LowerCaseFilterFactory\"}\n",
    "              ]}\n",
    "            }\n",
    "        }\"\"\",\n",
    "            \"\"\"{\n",
    "          \"add-field\":{\"name\":\"surface_form\", \"type\":\"string\", \"stored\":true}\n",
    "        }\"\"\",\n",
    "            \"\"\"{\n",
    "          \"add-field\":{\"name\":\"canonical_form\", \"type\":\"string\", \"stored\":true}\n",
    "        }\"\"\",\n",
    "            \"\"\"{\n",
    "          \"add-field\":{\"name\":\"name\", \"type\":\"text_general\"}\n",
    "        }\"\"\",\n",
    "            \"\"\"{\n",
    "          \"add-field\":{\"name\":\"popularity\", \"type\":\"pint\", \"stored\":true}\n",
    "        }\"\"\",\n",
    "            \"\"\"{\n",
    "          \"add-field\":{\"name\":\"name_tag\", \"type\":\"tag\", \"stored\":false}\n",
    "        }\"\"\",\n",
    "            \"\"\"{\n",
    "          \"add-copy-field\":{\"source\":\"name\", \"dest\":[\"surface_form\", \"name_tag\", \"canonical_form\"]}\n",
    "        }\"\"\",\n",
    "            \"\"\"{\n",
    "          \"add-copy-field\":{\"source\":\"population_i\", \"dest\":[\"popularity\"]}\n",
    "        }\"\"\",\n",
    "            \"\"\"{\n",
    "          \"add-copy-field\":{\"source\":\"surface_form\", \"dest\":[\"name_tag\"]}\n",
    "        }\"\"\"\n",
    "    ]\n",
    "    \n",
    "    for schemaCommand in schemaCommands:\n",
    "        response = requests.post(solr_url + collection + \"/schema\", headers=headers, data=schemaCommand)\n",
    "        #print(response)    \n",
    "    \n",
    "    \n",
    "    response = requests.post(solr_url + collection + \"/config\", headers=headers, data=\"\"\"{\n",
    "      \"add-requesthandler\" : {\n",
    "        \"name\": \"/tag\",\n",
    "        \"class\":\"solr.TaggerRequestHandler\",\n",
    "        \"defaults\":{\"field\":\"name_tag\"}\n",
    "      }\n",
    "    }\"\"\")\n",
    "    #print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_reviews_collection():\n",
    "    print(\"\\nLoading Reviews...\")\n",
    "    csvFile = \"../data/semantic-search/reviews.csv\"\n",
    "    reviews_collection = \"reviews\"\n",
    "    reviews_update_opts={\"zkhost\": \"aips-zk\", \"collection\": reviews_collection, \n",
    "                        \"gen_uniq_key\": \"true\", \"commit_within\": \"5000\"}\n",
    "    csvDF = spark.read.format(\"com.databricks.spark.csv\") \\\n",
    "        .option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"charset\", \"utf-8\").option(\"quote\", \"\\\"\") \\\n",
    "        .option(\"escape\", \"\\\"\").option(\"multiLine\",\"true\").option(\"delimiter\", \",\") \\\n",
    "        .load(csvFile) \\\n",
    "        .withColumn(\"poplarity_i\", col(\"stars_i\") * 20) \\\n",
    "        .select(\n",
    "          \"id\", \"name_t\", \"city_t\", \"state_t\", \"text_t\", \"stars_i\", \"popularity_i\", \n",
    "          \"categories_t\",  \"location_pt_s\", \"type_ss\", \"latitude_d\", \"longitude_d\")\n",
    "    csvDF.write.format(\"solr\").options(**reviews_update_opts).mode(\"overwrite\").save()\n",
    "    print(\"Reviews Schema: \")\n",
    "    csvDF.printSchema()\n",
    "    print(\"Status: Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index the Reviews Dataset into the Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiping 'reviews' collection\n",
      "[('action', 'CREATE'), ('name', 'reviews'), ('numShards', 1), ('replicationFactor', 1)]\n",
      "Creating reviews' collection\n",
      "Status: Success\n",
      "\n",
      "Loading Reviews...\n",
      "Reviews Schema: \n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name_t: string (nullable = true)\n",
      " |-- city_t: string (nullable = true)\n",
      " |-- state_t: string (nullable = true)\n",
      " |-- text_t: string (nullable = true)\n",
      " |-- stars_i: integer (nullable = true)\n",
      " |-- popularity_i: integer (nullable = true)\n",
      " |-- categories_t: string (nullable = true)\n",
      " |-- location_pt_s: string (nullable = true)\n",
      " |-- type_ss: string (nullable = true)\n",
      " |-- latitude_d: double (nullable = true)\n",
      " |-- longitude_d: double (nullable = true)\n",
      "\n",
      "Status: Success\n"
     ]
    }
   ],
   "source": [
    "create_reviews_collection()\n",
    "index_reviews_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enities Dataset (Manually-specified Knowledge Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entities_collection():\n",
    "    #Create Entities Collection\n",
    "    entities_collection=\"entities\"\n",
    "    #delete collection\n",
    "    requests.get(solr_url + \"admin/collections?action=DELETE&name=\" + entities_collection)\n",
    "    #delete configSet to start from scratch\n",
    "    requests.get(solr_url + \"admin/configs?action=DELETE&name=\" + entities_collection + \".AUTOCREATED\")\n",
    "\n",
    "    create_collection(entities_collection)\n",
    "    enable_text_tagger(entities_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_entities():\n",
    "    entities_collection=\"entities\"\n",
    "    print(\"Loading Entities...\")\n",
    "    csvFile = \"../data/semantic-search/entities-manual.csv\"\n",
    "    entities_update_opts={\"zkhost\": \"aips-zk\", \"collection\": entities_collection, \"gen_uniq_key\": \"true\", \"commit_within\": \"5000\"}\n",
    "    csvDF = spark.read.format(\"com.databricks.spark.csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"charset\", \"utf-8\") \\\n",
    "        .option(\"quote\", \"\\\"\") \\\n",
    "        .option(\"escape\", \"\\\\\") \\\n",
    "        .option(\"multiLine\",\"true\") \\\n",
    "        .option(\"delimiter\", \",\").load(csvFile)\n",
    "    csvDF.write.format(\"solr\").options(**entities_update_opts).mode(\"overwrite\").save()\n",
    "    print(\"Entities Schema: \")\n",
    "    csvDF.printSchema()\n",
    "    print(\"Status: Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cities Dataset (Geonames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Geonames...\n",
      "Entities Schema: \n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- ascii_name_s: string (nullable = true)\n",
      " |-- alternative_names_s: string (nullable = true)\n",
      " |-- latitude_s: string (nullable = true)\n",
      " |-- longitude_s: string (nullable = true)\n",
      " |-- feature_class_s: string (nullable = true)\n",
      " |-- feature_code_s: string (nullable = true)\n",
      " |-- StringType: string (nullable = true)\n",
      " |-- cc2_s: string (nullable = true)\n",
      " |-- admin_code_1_s: string (nullable = true)\n",
      " |-- admin_code_2_s: string (nullable = true)\n",
      " |-- admin_code_3_s: string (nullable = true)\n",
      " |-- admin_code_4_s: string (nullable = true)\n",
      " |-- population_i: integer (nullable = true)\n",
      " |-- elevation_s: string (nullable = true)\n",
      " |-- dem_s: string (nullable = true)\n",
      " |-- timezone_s: string (nullable = true)\n",
      " |-- modification_date_s: string (nullable = true)\n",
      " |-- type: string (nullable = false)\n",
      " |-- location_p: string (nullable = false)\n",
      "\n",
      "Status: Success\n"
     ]
    }
   ],
   "source": [
    "#Modify Schema to make some fields explicitly searchable by keyword\n",
    "#upsert_text_field(jobs_collection, \"company_country\")\n",
    "#upsert_text_field(jobs_collection, \"job_description\")\n",
    "#upsert_text_field(jobs_collection, \"company_description\")\n",
    "#upsert_text_field(products_collection, \"longDescription\")\n",
    "#upsert_text_field(products_collection, \"manufacturer\")\n",
    "\n",
    "def index_cities():\n",
    "    entities_collection=\"entities\"\n",
    "    print(\"Loading Geonames...\")\n",
    "    csvFile = \"../data/semantic-search/cities1000.txt\"\n",
    "    entities_update_opts={\"zkhost\": \"aips-zk\", \"collection\": entities_collection, \"gen_uniq_key\": \"true\", \"commit_within\": \"5000\"}\n",
    "\n",
    "    from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "    from pyspark.sql.functions import concat_ws\n",
    "\n",
    "    schema = StructType() \\\n",
    "          .add(\"id\",StringType(),True) \\\n",
    "          .add(\"name\",StringType(),True) \\\n",
    "          .add(\"ascii_name_s\",StringType(),True) \\\n",
    "          .add(\"alternative_names_s\",StringType(),True) \\\n",
    "          .add(\"latitude_s\",StringType(),True) \\\n",
    "          .add(\"longitude_s\",StringType(),True) \\\n",
    "          .add(\"feature_class_s\",StringType(),True) \\\n",
    "          .add(\"feature_code_s\",StringType(),True) \\\n",
    "          .add(\"StringType\",StringType(),True) \\\n",
    "          .add(\"cc2_s\",StringType(),True) \\\n",
    "          .add(\"admin_code_1_s\",StringType(),True) \\\n",
    "          .add(\"admin_code_2_s\",StringType(),True) \\\n",
    "          .add(\"admin_code_3_s\",StringType(),True) \\\n",
    "          .add(\"admin_code_4_s\",StringType(),True) \\\n",
    "          .add(\"population_i\",IntegerType(),True) \\\n",
    "          .add(\"elevation_s\",StringType(),True) \\\n",
    "          .add(\"dem_s\",StringType(),True) \\\n",
    "          .add(\"timezone_s\",StringType(),True) \\\n",
    "          .add(\"modification_date_s\",StringType(),True)\n",
    "\n",
    "    csvDF = spark.read.format(\"com.databricks.spark.csv\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"charset\", \"utf-8\") \\\n",
    "        .option(\"quote\", \"\\\"\") \\\n",
    "        .option(\"escape\", \"\\\\\") \\\n",
    "        .option(\"multiLine\",\"true\") \\\n",
    "        .option(\"delimiter\", \"\\t\") \\\n",
    "        .load(csvFile, schema=schema) \\\n",
    "        .withColumn(\"type\", lit(\"city\")) \\\n",
    "        .withColumn(\"location_p\", concat_ws(\",\", \"latitude_s\", \"longitude_s\"))\n",
    "        #.show()\n",
    "\n",
    "    csvDF.write.format(\"solr\").options(**entities_update_opts).mode(\"overwrite\").save()\n",
    "    print(\"Entities Schema: \")\n",
    "    csvDF.printSchema()\n",
    "    print(\"Status: Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the Reviews Search Web Server and Launching the Search Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_running_webservers():\n",
    "    already_running_webservers = ! ps -ef | grep '[s]tart-webserver.py' | awk '{print $2}'\n",
    "    return already_running_webservers\n",
    "    \n",
    "def stop_running_webservers():\n",
    "    already_running_webservers = get_running_webservers()\n",
    "    for pid in already_running_webservers:\n",
    "        print(\"Stopping webserver (pid: \" + pid + \")\")\n",
    "        results = ! xargs kill -9 {pid}\n",
    "\n",
    "def start_reviews_search_webserver():\n",
    "    stop_running_webservers() #in case it was already running\n",
    "    ! pip install staticmap\n",
    "    get_ipython().system = os.system\n",
    "    ! cd ../webserver && python start-webserver.py &\n",
    "    if len(get_running_webservers()) > 0:\n",
    "        print(\"Successfully Started Webserver (pid: \" + get_running_webservers()[0] + \")!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting staticmap\n",
      "  Downloading staticmap-0.5.5.tar.gz (5.6 kB)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from staticmap) (7.1.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from staticmap) (2.22.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->staticmap) (2019.6.16)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->staticmap) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->staticmap) (1.25.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->staticmap) (2.8)\n",
      "Building wheels for collected packages: staticmap\n",
      "  Building wheel for staticmap (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for staticmap: filename=staticmap-0.5.5-py3-none-any.whl size=6460 sha256=508182f16ea939a507398d81a3b8080d5e30eae6b270da6819ff91a58c54da16\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/26/a1/4d/0c9eff264ca4fba0b3a4d66e62f5d53f21471cb200719a9aba\n",
      "Successfully built staticmap\n",
      "Installing collected packages: staticmap\n",
      "Successfully installed staticmap-0.5.5\n",
      "Successfully Started Webserver (pid: 145)!\n"
     ]
    }
   ],
   "source": [
    "#Start the web server\n",
    "start_reviews_search_webserver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"http://localhost:2345/search?q=bbq\" width=\"100%\" height=\"800\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"http://localhost:2345/search?q=bbq\" width=\"100%\" height=\"800\"></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping webserver (pid: 1005)\n"
     ]
    }
   ],
   "source": [
    "#Cleanup so webserver doesn't keep running after you're done\n",
    "stop_running_webservers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success!\n",
    "\n",
    "Now that you've indexed several large text datasets, in the next notebook we will explore the rich graph of semantic relationships embedded within those documents by leveraging Semantic Knowledge Graphs for real-time traversal and ranking of arbitrary relationships within the domains of our datasets.\n",
    "\n",
    "Up next: [Working with Semantic Knowledge Graphs](3.semantic-knowledge-graph.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
